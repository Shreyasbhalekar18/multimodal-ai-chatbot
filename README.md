# ğŸ¤– Multimodal Chatbot System

> A smart AI-powered chatbot that supports PDF, image, and audio inputs using GenAI, Whisper, CLIP, and other state-of-the-art models.

## ğŸ§  Overview

This project is a **Multimodal Chatbot System** designed to understand and respond to **text**, **PDFs**, **images**, and **audio**. It combines the capabilities of various advanced AI models like:

- **LLMs (e.g., GPT-4)** for natural language conversations.
- **Whisper** for speech-to-text.
- **CLIP** for image understanding.
- **LangChain** and **FAISS** for document-based memory and semantic search.

---

## ğŸ“¦ Features

âœ… Accepts text, image (JPG/PNG), PDF, and audio (MP3/WAV)  
âœ… Transcribes audio using Whisper  
âœ… Extracts text & answers from PDFs  
âœ… Understands visual content using CLIP  
âœ… Stores and retrieves user interactions using Vector DB  
âœ… Streamlit or Gradio frontend interface  
âœ… Modular backend with FastAPI / Flask

---

## ğŸ—‚ï¸ Project Structure

multimodal-chatbot/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py # FastAPI or Flask server
â”‚ â”œâ”€â”€ handlers/
â”‚ â”‚ â”œâ”€â”€ text_handler.py
â”‚ â”‚ â”œâ”€â”€ pdf_handler.py
â”‚ â”‚ â”œâ”€â”€ image_handler.py
â”‚ â”‚ â”œâ”€â”€ audio_handler.py
â”‚ â”‚ â””â”€â”€ vector_store.py
â”‚ â””â”€â”€ models/
â”‚ â”œâ”€â”€ llm_interface.py
â”‚ â”œâ”€â”€ whisper_model.py
â”‚ â””â”€â”€ clip_model.py
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ app.py # Streamlit or Gradio frontend
â”‚ â””â”€â”€ components/
â”‚ â”œâ”€â”€ chat_ui.py
â”‚ â””â”€â”€ uploader.py
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ uploaded_files/ # Stores uploaded PDFs/images/audio
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

yaml
Copy
Edit

---

## ğŸš€ Getting Started

### ğŸ”§ Prerequisites

- Python 3.9+
- Virtual environment (`venv` or `conda`)
- GPU (optional, for Whisper and CLIP)

### ğŸ“¥ Installation

```bash
git clone https://github.com/your-username/multimodal-chatbot.git
cd multimodal-chatbot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
ğŸ› ï¸ Run the App
ğŸ–¥ï¸ Backend (FastAPI)
bash
Copy
Edit
cd backend
uvicorn app:app --reload
ğŸ§‘â€ğŸ’» Frontend (Streamlit or Gradio)
bash
Copy
Edit
cd frontend
streamlit run app.py
# OR
python app.py  # If using Gradio
ğŸ” How It Works
Input Type	Processing Model	Output
Text	GPT-4 / OpenAI API	Chat response
PDF	LangChain + FAISS	Answers to questions from documents
Image	OpenAI CLIP	Description, captioning, and context
Audio	Whisper (open-source or OpenAI API)	Transcribed text

ğŸ§  Models Used
OpenAI GPT-4 / GPT-3.5 â€“ Text generation

Whisper â€“ Speech recognition

CLIP â€“ Vision-language understanding

LangChain + FAISS â€“ PDF semantic QA

âœ¨ Future Scope
ğŸ” Add user authentication

ğŸŒ Multilingual support

ğŸ§  Connect to long-term vector memory (e.g., Pinecone)

ğŸ’¬ Context-aware history

ğŸ“± Deploy to Hugging Face Spaces or Streamlit Cloud

ğŸ¤ Contributors
Shreyas Bhalekar â€“ AI & Full Stack Developer

ğŸ“ License
MIT License. See LICENSE file for more info.

ğŸ“¸ Screenshots
Chat UI	Image Upload	Audio Transcription
âœ…	ğŸ–¼ï¸	ğŸ™ï¸

ğŸ™‹ FAQ
Q: Can I use this for commercial projects?
Yes, but make sure to follow the licenses of the third-party models (OpenAI, Whisper, etc.).

Q: Does it support real-time voice chat?
No, but you can build that using Whisper + TTS like Coqui.ai.

ğŸ” Built with care to bridge the gap between humans and machines, one modality at a time.

